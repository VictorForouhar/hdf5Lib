{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "238b2988-1b8d-45d6-85e5-1ae6b51a71cc",
   "metadata": {},
   "source": [
    "Importing module \n",
    "======="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5681241-5b4d-43a1-8d0d-9c4fd2a6d964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../hdf5Lib')\n",
    "import hdf5Lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47438f31-43e1-4614-bbf3-d6431681063e",
   "metadata": {},
   "source": [
    "Initialising class\n",
    "======="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeffc81-3876-4994-829b-1b498e664ef5",
   "metadata": {},
   "source": [
    "Let's first specify the paths to the example files. For this example we have two folders:\n",
    "* /examples/single_file/file.hdf5\n",
    "* /examples/split_files/subfile_??.hdf5\n",
    "\n",
    "The former is a single file that contains all of the data, which in this example is simply a numpy array holding integers from 0 to 499, i.e. np.arange(500). The latter folder contains 50 files, each containing one tenth of the data, i.e.:\n",
    "* /examples/split_files/subfile_00.hdf5 --> np.arange(0,10)\n",
    "* /examples/split_files/subfile_01.hdf5 --> np.arange(10,20)\n",
    "* ...\n",
    "* /examples/split_files/subfile_49.hdf5 --> np.arange(490,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552c3eca-0298-4948-b282-e09697b05e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to single file example\n",
    "single_file_path    = '../examples/single_file/file.hdf5'\n",
    "\n",
    "# Alternative ways of specifying paths to the split files \n",
    "split_file_path_v1 = '../examples/split_files/subfile_%.2d.hdf5'\n",
    "split_file_path_v2 = ['../examples/split_files/subfile_%.2d.hdf5'%i for i in range(50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eb8d36-e509-41a1-81f6-7dccd924d9f4",
   "metadata": {},
   "source": [
    "When we initialise the class, we should specify whether the files have been split (is_split) and if we are using string formatting (i.e. split_file_path_v1), then the number of files the data has been split into. In this case, it is 50. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c9ed7d-b6f2-42ba-9dc8-5724439c0fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_file   = hdf5Lib.read_hdf5(single_file_path, is_split=False)\n",
    "split_file_v1 = hdf5Lib.read_hdf5(split_file_path_v1, is_split=True, number_subfiles=50)\n",
    "split_file_v2 = hdf5Lib.read_hdf5(split_file_path_v2, is_split=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27299d8-eb9e-414b-92b5-4c58effa35d2",
   "metadata": {},
   "source": [
    "Note that either way of defining the path to the split files works:\n",
    "* Formatted string: one needs to specify the number of subfiles\n",
    "* List of strings: each entry should be the path to an individual file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e97f91c-1250-46cc-9410-6831ea5053fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert split_file_v1.file_list ==  split_file_v2.file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5383c874-6133-4001-aba8-435c0973c962",
   "metadata": {},
   "source": [
    "Exploring hdf5 file\n",
    "=======\n",
    "We can now have a look at the contents of the hdf5 files... Let's check what groups the single file hdf5 contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259f9e4a-bdc7-46fe-811a-8503f7fa7e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_file.print_entries()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9591dd5e-f8de-4728-b574-982f2e65132c",
   "metadata": {},
   "source": [
    "Again, we can check whether 'group_a' contains any groups and/or datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5e9ffe-1e5f-4fd7-b08f-5a4662d790e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_file.print_entries('group_a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43586dec-7749-4674-8644-8aecb98b63f8",
   "metadata": {},
   "source": [
    "What about attributes? Let's see those in 'group_a/dataset_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a589a1-b270-4da4-8658-9b368736766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_file.print_attributes('group_a/dataset_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae4983a-65d0-42f7-bc7a-05846496cc6e",
   "metadata": {},
   "source": [
    "So what is the value of the Hubble parameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c49f9c-709c-455e-90f9-247148dfbd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_file.get_attribute('group_a/dataset_1','hubbleParameter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a800cc-6b26-4d3c-846f-fa20571dd3b2",
   "metadata": {},
   "source": [
    "And of pi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bdcb2d-09b0-46cc-bd58-5251a0256ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_file.get_attribute('group_a/dataset_1','pi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23c83d3-710a-48db-a016-da2815bf07b3",
   "metadata": {},
   "source": [
    "It works exactly the same for the split files, except that only one of the subfiles is considered when retrieving the information (by default the first one) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051415de-19b7-45c5-80ca-3a30fde7f868",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_file_v1.print_entries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1887bb93-52bd-432e-b1cd-aafc09d7ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_file_v1.print_entries('group_a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db85625e-a500-4a6c-89e3-b47fb4a5e788",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_file_v1.print_attributes('group_a/dataset_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bce11d-ba43-4dfc-8d42-f9fb485652ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_file_v1.get_attribute('group_a/dataset_1','pi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f2a38b-7f72-4c40-bada-18a5fba58bc9",
   "metadata": {},
   "source": [
    "Loading data\n",
    "=======\n",
    "This can be done easily by loading data sequentially (self.get_data) or in parallel (self.get_data_parallel). When using parallel loading, one can specify the number of cores to use via number_workers (defaults to all available ones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e65262-006a-47d4-8bcf-8b83d54a7ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_file.get_data('group_a/dataset_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7792ce66-d5ee-4b12-affa-d53fb9d83218",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_file.get_data_parallel('group_a/dataset_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462bb909-ba35-4f88-99e1-7ab17bc2daca",
   "metadata": {},
   "source": [
    "Note that this class joins all the data that was previously split over multiple files into a single array, as if it was loaded from a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9c5024-1461-4d7a-952d-076df6067f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (split_file_v1.get_data('group_a/dataset_1') == single_file.get_data('group_a/dataset_1')).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b37d60-d975-4673-8108-35a03f7c63c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (split_file_v1.get_data_parallel('group_a/dataset_1') == single_file.get_data_parallel('group_a/dataset_1')).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
