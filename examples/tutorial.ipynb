{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "238b2988-1b8d-45d6-85e5-1ae6b51a71cc",
   "metadata": {},
   "source": [
    "Importing module \n",
    "======="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5681241-5b4d-43a1-8d0d-9c4fd2a6d964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdf5Lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47438f31-43e1-4614-bbf3-d6431681063e",
   "metadata": {},
   "source": [
    "Initialising class\n",
    "======="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeffc81-3876-4994-829b-1b498e664ef5",
   "metadata": {},
   "source": [
    "Let's first specify the paths to the example files. For this example we have two folders:\n",
    "* /examples/single_file/file.hdf5\n",
    "* /examples/split_files/subfile_??.hdf5\n",
    "\n",
    "The former is a single file that contains all of the data, which in this example is simply a numpy array holding integers from 0 to 499, i.e. np.arange(500). The latter folder contains 50 files, each containing one tenth of the data, i.e.:\n",
    "* /examples/split_files/subfile_00.hdf5 --> np.arange(0,10)\n",
    "* /examples/split_files/subfile_01.hdf5 --> np.arange(10,20)\n",
    "* ...\n",
    "* /examples/split_files/subfile_49.hdf5 --> np.arange(490,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552c3eca-0298-4948-b282-e09697b05e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to single file example\n",
    "single_file_path    = '../examples/single_file/file.hdf5'\n",
    "\n",
    "# Alternative ways of specifying paths to the split files \n",
    "split_file_path_v1 = '../examples/split_files/subfile_%.2d.hdf5'\n",
    "split_file_path_v2 = ['../examples/split_files/subfile_%.2d.hdf5'%i for i in range(50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eb8d36-e509-41a1-81f6-7dccd924d9f4",
   "metadata": {},
   "source": [
    "When we initialise the class, we should specify whether the files have been split (is_split) and if we are using string formatting (i.e. split_file_path_v1), then the number of files the data has been split into. In this case, it is 50. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c9ed7d-b6f2-42ba-9dc8-5724439c0fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_file   = hdf5Lib.Read(single_file_path)\n",
    "split_file_v1 = hdf5Lib.Read(split_file_path_v1, number_files=50)\n",
    "split_file_v2 = hdf5Lib.Read(split_file_path_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27299d8-eb9e-414b-92b5-4c58effa35d2",
   "metadata": {},
   "source": [
    "Note that either way of defining the path to the split files works:\n",
    "* Formatted string: one needs to specify the number of subfiles\n",
    "* List of strings: each entry should be the path to an individual file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e97f91c-1250-46cc-9410-6831ea5053fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert split_file_v1._file_list ==  split_file_v2._file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5383c874-6133-4001-aba8-435c0973c962",
   "metadata": {},
   "source": [
    "Exploring hdf5 file\n",
    "=======\n",
    "We can now have a look at the contents of the hdf5 files... Let's check what groups the single file hdf5 contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259f9e4a-bdc7-46fe-811a-8503f7fa7e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_file.print_entries()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9591dd5e-f8de-4728-b574-982f2e65132c",
   "metadata": {},
   "source": [
    "Again, we can check whether 'group_a' contains any groups and/or datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5e9ffe-1e5f-4fd7-b08f-5a4662d790e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_file.print_entries('group_a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43586dec-7749-4674-8644-8aecb98b63f8",
   "metadata": {},
   "source": [
    "What about attributes? Let's see those in 'group_a/dataset_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a589a1-b270-4da4-8658-9b368736766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_file.print_attributes('group_a/dataset_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae4983a-65d0-42f7-bc7a-05846496cc6e",
   "metadata": {},
   "source": [
    "So what are their values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c49f9c-709c-455e-90f9-247148dfbd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('hubbleParameter:' ,single_file.get_attribute('group_a/dataset_1','hubbleParameter'))\n",
    "print ('pi:' ,single_file.get_attribute('group_a/dataset_1','pi'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23c83d3-710a-48db-a016-da2815bf07b3",
   "metadata": {},
   "source": [
    "It works exactly the same for the split files, except that only one of the subfiles is considered when retrieving the information (by default the first one) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051415de-19b7-45c5-80ca-3a30fde7f868",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_file_v1.print_entries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1887bb93-52bd-432e-b1cd-aafc09d7ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_file_v1.print_entries('group_a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db85625e-a500-4a6c-89e3-b47fb4a5e788",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_file_v1.print_attributes('group_a/dataset_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bce11d-ba43-4dfc-8d42-f9fb485652ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('hubbleParameter:' ,split_file_v1.get_attribute('group_a/dataset_1','hubbleParameter'))\n",
    "print ('pi:' ,split_file_v1.get_attribute('group_a/dataset_1','pi'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f2a38b-7f72-4c40-bada-18a5fba58bc9",
   "metadata": {},
   "source": [
    "Loading data\n",
    "=======\n",
    "This can be done easily by calling the __get_item__ method of the class. Data can be loaded sequentially or in parallel, which is determined during class initialisation via the flag parallel. By default, parallel loading is always enabled, except for when there is only a single hdf5 file (not implemented)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261f9394-51dd-41fa-b3b2-437c95a66c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if opened in parallel\n",
    "print ('Single file opened in parallel mode?', single_file._parallel)\n",
    "print ('Multiple file opened in parallel mode?', split_file_v1._parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e65262-006a-47d4-8bcf-8b83d54a7ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from single file\n",
    "data_single_file = single_file['group_a/dataset_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7792ce66-d5ee-4b12-affa-d53fb9d83218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from split file\n",
    "data_split_file = split_file_v1['group_a/dataset_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462bb909-ba35-4f88-99e1-7ab17bc2daca",
   "metadata": {},
   "source": [
    "Note that this class joins all the data that was previously split over multiple files into a single array, as if it was loaded from a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9c5024-1461-4d7a-952d-076df6067f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (data_single_file == data_split_file).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d595d3-6002-4732-a33c-686e2bb03333",
   "metadata": {},
   "source": [
    "Finally, before the load methods are called, the code checks whether the requested dataset has been already loaded (i.e. in self._data). This prevents spending time loading the same data over and over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a8fff4-3856-49fb-a095-0e31cdd4c9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_file_v1._data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_kernel",
   "language": "python",
   "name": "my_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
